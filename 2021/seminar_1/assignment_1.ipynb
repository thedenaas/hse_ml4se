{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Explore solutions of linear regression model with MSE loss.  \n",
    "Investigate how regularization affects the solution.  \n",
    "In this toy example we use simulated data and select $|| \\hat w - w||_2$ as quality metric (distance between found solution and the ground truth).  \n",
    "In the tasks 1-4 you are allowed to use only `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition number of matrix $A$ is \n",
    "$$ k(A) = \\frac {\\lambda_{max}(A)} {\\lambda_{min}(A)}$$\n",
    "where  \n",
    "$\\lambda_{max}$ - max eigenvalue of $A$  \n",
    "$\\lambda_{min}$ - min eigenvalue of $A$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k(A) 1902233125.7296236\n"
     ]
    }
   ],
   "source": [
    "X, y, coef = make_regression(n_samples=1000, \n",
    "                             n_features=1000, \n",
    "                             n_informative=1000, \n",
    "                             n_targets=1, \n",
    "                             bias=0.0, \n",
    "                             effective_rank=10, \n",
    "                             tail_strength=0.5, \n",
    "                             noise=0.1, \n",
    "                             shuffle=True, coef=True, random_state=42)\n",
    "\n",
    "print('k(A)', np.linalg.cond(X.T.dot(X)))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "coef = scaler.inverse_transform(coef.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (2 points)\n",
    "Implement analytic solution for linear regression with MSE loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def solve(X, y):\n",
    "    \"\"\"\n",
    "    @return: weights of the linear model\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    w = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    return w\n",
    "\n",
    "if print(np.linalg.norm(solve(X, y) - coef) < 400):\n",
    "    print('success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (2 points)\n",
    "Implement analytic solution for linear regression with MSE loss and $L_2$ regularization.  \n",
    "Plot the dependence between regularization coefficient $\\alpha$ and $|| \\hat w - w||_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(X, y, alpha):\n",
    "    \"\"\"\n",
    "    @return: weights of the linear model\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    w = ...\n",
    "    return w\n",
    "\n",
    "if print(np.linalg.norm(solve(X, y, 0.1) - coef) < 10):\n",
    "    print('success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (2 points)\n",
    "Implement Full Gradient Descent solution for linear MSE regression with $L_2$ regularization.  \n",
    "Use gradient norm for stopping criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(X, y, alpha, max_iter, tol):\n",
    "    \"\"\"\n",
    "    @param tol: value for stopping criterion\n",
    "    @param max_iter: max number of iterations\n",
    "    @return: weights of the linear model\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    w = ...\n",
    "    return w\n",
    "\n",
    "if print(np.linalg.norm(solve(X, y, 0.99, 1000, 0.001) - coef) < 10):\n",
    "    print('success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (2 points)\n",
    "What param in `make_regression` affects condition number of $X^T X$ the most? Why?    \n",
    "Tweak `make_regression` routine to generate problems with different condition numbers.  \n",
    "Plot the dependence between $||\\hat w - w||_2$ of the analytic solution from `task 1` and condition number of $X^T X$.  \n",
    "Use log scale for condition numbers in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 (2 points)\n",
    "How does switching on and off the `StandardScaler` transformation affects quality of solutions in the tasks 1-3?  \n",
    "How it is connected with $L_2$ norm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
