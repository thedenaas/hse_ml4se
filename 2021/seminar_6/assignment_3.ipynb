{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3. Language Models and Text Generation\n",
    "\n",
    "**[3 points]** Train language model, that generates texts from Wikipedia.   \n",
    "Use [WikiText-2](https://huggingface.co/datasets/wikitext/viewer/wikitext-2-raw-v1/train) dataset, also available in [`datasets`](https://huggingface.co/docs/datasets/index.html).  \n",
    "\n",
    "Use [`tokenizers`](https://huggingface.co/docs/tokenizers/python/latest/) library from Huggingface for text tokenization. \n",
    "\n",
    "Pay attention to vocab size, probably subword tokens are better.    \n",
    "\n",
    "Your model should be an autogressive LSTM or GRU.  \n",
    "\n",
    "**[1 point]** Use [catalyst](https://catalyst-team.github.io/catalyst/) for model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Explore several text generation techniques with signatures provided below:\n",
    "1. **[1 point]** Argmax\n",
    "1. **[2 points]** Beam Search\n",
    "1. **[1 point]** Sampling with temperature\n",
    "1. **[1 point]** Top-K\n",
    "1. **[1 point]** Top-P\n",
    "\n",
    "\n",
    "Text generation should be terminated when either **max length** is reached or **\\<eos\\>** is generated.  \n",
    "\n",
    "For every text generation method you should provide generated examples. \n",
    "Look at the title formatting of articles in the dataset.\n",
    "Try to condition generated text on old and new article titles.\n",
    "Each example must contain at least 10 generated words (not subword tokens).\n",
    "\n",
    "Readings on text generation techniques:\n",
    "https://arxiv.org/abs/1904.09751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def arg_max(model: torch.nn.Module, input_str: List[int], max_len: int):\n",
    "    \"\"\"\n",
    "    :param model: language model\n",
    "    :param input_str: tokenized string on which generated text is conditioned on\n",
    "    :param max_len: max number of tokens to generate\n",
    "    :return List(int): returns generated string tokens\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(model: torch.nn.Module, input_str: List[int], beam_size: int, max_len: int):\n",
    "    \"\"\"\n",
    "    :param model: language model\n",
    "    :param input_str: tokenized string on which generated text is conditioned on\n",
    "    :param beam_size: beam size\n",
    "    :param max_len: max number of tokens to generate\n",
    "    :return List[Tuple(List(int), float)]: return pairs of generated string tokens and corresponding log probability\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (3842260837.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_4920/3842260837.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def beam_search(model, input_str, beam_size, max_len):\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def sample_t(model: torch.nn.Module, input_str: List[int], t: float, max_len: int):\n",
    "    \"\"\"\n",
    "    Sampling with probability distribution with temperature\n",
    "    :param model: language model\n",
    "    :param input_str: tokenized string on which generated text is conditioned on\n",
    "    :param t: temperature\n",
    "    :param max_len: max number of tokens to generate\n",
    "    :return List(int): generated string tokens\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_t(model: torch.nn.Module, input_str: List[int], t: float, max_len: int):\n",
    "    \"\"\"\n",
    "    Sampling with probability distribution with temperature\n",
    "    :param model: language model\n",
    "    :param input_str: tokenized string on which generated text is conditioned on\n",
    "    :param t: temperature\n",
    "    :param max_len: max number of tokens to generate\n",
    "    :return List(int): generated string tokens\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_topk(model: torch.nn.Module, input_str: List[int], k: int, max_len: int):\n",
    "    \"\"\"\n",
    "    :param model: language model\n",
    "    :param input_str: tokenized string on which generated text is conditioned on\n",
    "    :param k: number of tokens sorted by probability in the descending order\n",
    "    :param max_len: max number of tokens to generate\n",
    "    :return List(int): generated string tokens\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample_topp(model: torch.nn.Module, input_str: List[int], p: float, max_len: int):\n",
    "    \"\"\"\n",
    "    :param model: language model\n",
    "    :param input_str tokenized string on which generated text is conditioned on\n",
    "    :param p: token probability mass\n",
    "    :param max_len: max number of tokens to generate\n",
    "    :return List(int): generated string tokens\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
